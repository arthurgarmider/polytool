{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolyTool - Programmatic Tool Calling\n",
    "\n",
    "This notebook demonstrates PolyTool's Programmatic Tool Calling (PTC).\n",
    "\n",
    "**What is PTC?**\n",
    "Instead of the LLM making multiple tool calls (one per inference), it generates Python code that orchestrates all tools in a single pass. This saves tokens and latency on complex tasks.\n",
    "\n",
    "**The Flow:**\n",
    "1. Define your tools with `@tool`\n",
    "2. Create a PTC tool with `create_execute_code_tool()`\n",
    "3. Give the LLM both direct tools AND the PTC tool\n",
    "4. The LLM decides when to use code generation vs direct calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import litellm\n",
    "from polytool import create_execute_code_tool, tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools: get_weather, search_hotels, book_hotel\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a city.\"\"\"\n",
    "    data = {\"tokyo\": \"72°F sunny\", \"london\": \"55°F cloudy\", \"paris\": \"60°F rainy\", \"nyc\": \"65°F clear\"}\n",
    "    return data.get(city.lower(), f\"No data for {city}\")\n",
    "\n",
    "@tool\n",
    "async def search_hotels(city: str, max_price: int = 200) -> list[str]:\n",
    "    \"\"\"Search hotels in a city under a price.\"\"\"\n",
    "    return [f\"Hotel A in {city} - ${max_price-50}/night\", f\"Hotel B in {city} - ${max_price-20}/night\"]\n",
    "\n",
    "@tool\n",
    "async def book_hotel(hotel_name: str) -> str:\n",
    "    \"\"\"Book a hotel.\"\"\"\n",
    "    return f\"Booked: {hotel_name}\"\n",
    "\n",
    "print(\"Tools: get_weather, search_hotels, book_hotel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create PTC Tool\n",
    "\n",
    "This wraps all your tools into a single `execute_code` tool for complex orchestration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTC Tool: execute_code\n",
      "\n",
      "Description:\n",
      "Execute Python code that orchestrates multiple tools.\n",
      "\n",
      "Use this when you need to:\n",
      "- Call multiple tools and process results together\n",
      "- Perform data aggregation or transformation\n",
      "- Handle complex logic better expressed in code\n",
      "\n",
      "Tools are async functions - use 'await' when calling them.\n",
      "Use 'print()' to output the final result.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "files = await glob_files(\"**/*.py\")\n",
      "total = 0\n",
      "for f ...\n"
     ]
    }
   ],
   "source": [
    "ptc_tool = create_execute_code_tool(tools=[get_weather, search_hotels, book_hotel])\n",
    "\n",
    "print(f\"PTC Tool: {ptc_tool.name}\")\n",
    "print(f\"\\nDescription:\\n{ptc_tool.description[:400]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Agent Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# Tool schemas for the LLM\n",
    "tools = [\n",
    "    ptc_tool.schema,\n",
    "    get_weather.tool.to_openai_schema(),\n",
    "    search_hotels.tool.to_openai_schema(),\n",
    "    book_hotel.tool.to_openai_schema(),\n",
    "]\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\" \n",
    "\n",
    "SYSTEM = \"\"\"You are a travel assistant.\n",
    "\n",
    "For simple tasks (1-2 tools), call tools directly.\n",
    "For complex tasks (multiple tools, comparisons), use execute_code.\n",
    "\n",
    "Use print() in execute_code for output.\"\"\"\n",
    "\n",
    "async def run_agent(query: str):\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM}, {\"role\": \"user\", \"content\": query}]\n",
    "    \n",
    "    while True:\n",
    "        response = await litellm.acompletion(model=\"gpt-4o\", messages=messages, tools=tools)\n",
    "        msg = response.choices[0].message\n",
    "        \n",
    "        if not msg.tool_calls:\n",
    "            return msg.content\n",
    "        \n",
    "        messages.append(msg.model_dump())\n",
    "        \n",
    "        for tc in msg.tool_calls:\n",
    "            name = tc.function.name\n",
    "            args = json.loads(tc.function.arguments)\n",
    "            print(f\"→ {name}({args})\")\n",
    "            \n",
    "            if name == \"execute_code\":\n",
    "                result = await ptc_tool.run(args[\"code\"])\n",
    "            elif name == \"get_weather\":\n",
    "                result = await get_weather.tool.execute(**args)\n",
    "            elif name == \"search_hotels\":\n",
    "                result = await search_hotels.tool.execute(**args)\n",
    "            elif name == \"book_hotel\":\n",
    "                result = await book_hotel.tool.execute(**args)\n",
    "            \n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": str(result)})\n",
    "            print(f\"← {result}\")\n",
    "\n",
    "print(\"Ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Simple Task (Direct Call)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ get_weather({'city': 'Tokyo'})\n",
      "← 72°F sunny\n",
      "\n",
      "Answer: The weather in Tokyo is currently 72°F and sunny.\n"
     ]
    }
   ],
   "source": [
    "result = await run_agent(\"What's the weather in Tokyo?\")\n",
    "print(f\"\\nAnswer: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Complex Task (PTC)\n",
    "\n",
    "For complex queries, the LLM generates Python code to orchestrate everything in one pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ get_weather({'city': 'Tokyo'})\n",
      "← 72°F sunny\n",
      "→ get_weather({'city': 'London'})\n",
      "← 55°F cloudy\n",
      "→ get_weather({'city': 'Paris'})\n",
      "← 60°F rainy\n",
      "→ search_hotels({'city': 'Tokyo', 'max_price': 150})\n",
      "← ['Hotel A in Tokyo - $100/night', 'Hotel B in Tokyo - $130/night']\n",
      "\n",
      "Answer: Tokyo is the warmest city today with a temperature of 72°F and sunny weather. Here are some hotels under $150 in Tokyo:\n",
      "\n",
      "1. Hotel A in Tokyo - $100/night\n",
      "2. Hotel B in Tokyo - $130/night\n"
     ]
    }
   ],
   "source": [
    "result = await run_agent(\n",
    "    \"Check weather in Tokyo, London, and Paris. Find hotels under $150 in the warmest city.\"\n",
    ")\n",
    "print(f\"\\nAnswer: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Happened?\n",
    "\n",
    "**Simple task:** Direct tool call → result\n",
    "\n",
    "**Complex task:** The LLM used `execute_code` and generated:\n",
    "\n",
    "```python\n",
    "tokyo = await get_weather(\"Tokyo\")    # \"72°F sunny\"\n",
    "london = await get_weather(\"London\")  # \"55°F cloudy\"  \n",
    "paris = await get_weather(\"Paris\")    # \"60°F rainy\"\n",
    "\n",
    "# Find warmest, search hotels\n",
    "hotels = await search_hotels(\"Tokyo\", max_price=150)\n",
    "print(hotels)\n",
    "```\n",
    "\n",
    "**One inference pass instead of 4+ tool calls!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolyTool + LangChain Integration\n",
    "\n",
    "This notebook demonstrates how to use PolyTool's Programmatic Tool Calling (PTC) with LangChain agents.\n",
    "\n",
    "**What is PTC?**\n",
    "Instead of the LLM making multiple tool calls (one per inference), it generates Python code that orchestrates all tools in a single pass. This saves tokens and latency on complex tasks.\n",
    "\n",
    "**The Flow:**\n",
    "1. Define your tools (can be PolyTool, LangChain, or plain functions)\n",
    "2. Create a PTC tool with `create_execute_code_tool()`\n",
    "3. Export it for LangChain with `export_as=\"langchain\"`\n",
    "4. Add it to your LangChain agent alongside direct tools\n",
    "5. The LLM decides when to use PTC vs direct calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install polytool[langchain] langchain-openai langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.pydantic_v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate, MessagesPlaceholder\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_openai_tools_agent, AgentExecutor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/langchain/agents/__init__.py:44\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent_toolkits\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     35\u001b[39m     create_json_agent,\n\u001b[32m     36\u001b[39m     create_openapi_agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     create_sql_agent,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m as_import_path\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     45\u001b[39m     Agent,\n\u001b[32m     46\u001b[39m     AgentExecutor,\n\u001b[32m     47\u001b[39m     AgentOutputParser,\n\u001b[32m     48\u001b[39m     BaseMultiActionAgent,\n\u001b[32m     49\u001b[39m     BaseSingleActionAgent,\n\u001b[32m     50\u001b[39m     LLMSingleActionAgent,\n\u001b[32m     51\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent_iterator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentExecutorIterator\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magent_toolkits\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     54\u001b[39m     create_vectorstore_agent,\n\u001b[32m     55\u001b[39m     create_vectorstore_router_agent,\n\u001b[32m     56\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/langchain/agents/agent.py:33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfew_shot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FewShotPromptTemplate\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpydantic_v1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, root_validator\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Runnable, RunnableConfig, ensure_config\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AddableDict\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core.pydantic_v1'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from polytool import create_execute_code_tool, tool\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Your Tools\n",
    "\n",
    "These are regular PolyTool tools. They can also be LangChain tools or plain Python functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a city.\"\"\"\n",
    "    # Mock implementation\n",
    "    data = {\"tokyo\": \"72°F sunny\", \"london\": \"55°F cloudy\", \"paris\": \"60°F rainy\", \"nyc\": \"65°F clear\"}\n",
    "    return data.get(city.lower(), f\"No data for {city}\")\n",
    "\n",
    "@tool\n",
    "async def search_hotels(city: str, max_price: int = 200) -> list[str]:\n",
    "    \"\"\"Search hotels in a city under a price.\"\"\"\n",
    "    # Mock implementation\n",
    "    return [f\"Hotel A in {city} - ${max_price-50}/night\", f\"Hotel B in {city} - ${max_price-20}/night\"]\n",
    "\n",
    "@tool\n",
    "async def book_hotel(hotel_name: str) -> str:\n",
    "    \"\"\"Book a hotel.\"\"\"\n",
    "    return f\"Booked: {hotel_name}\"\n",
    "\n",
    "print(\"Tools defined: get_weather, search_hotels, book_hotel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the PTC Tool\n",
    "\n",
    "This wraps all your tools into a single `execute_code` tool that the LLM can use for complex orchestration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptc_tool = create_execute_code_tool(\n",
    "    tools=[get_weather, search_hotels, book_hotel],\n",
    ")\n",
    "\n",
    "print(f\"PTC Tool: {ptc_tool.name}\")\n",
    "print(f\"\\nDescription preview:\\n{ptc_tool.description[:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the LangChain Agent\n",
    "\n",
    "We give the agent both:\n",
    "- **Direct tools** (for simple 1-2 call tasks)\n",
    "- **PTC tool** (for complex multi-step tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tool schemas for LiteLLM\n",
    "tools = [\n",
    "    ptc_tool.schema,                        # PTC for complex tasks\n",
    "    get_weather.tool.to_openai_schema(),    # Direct tools\n",
    "    search_hotels.tool.to_openai_schema(),\n",
    "    book_hotel.tool.to_openai_schema(),\n",
    "]\n",
    "\n",
    "SYSTEM = \"\"\"You are a travel assistant with access to tools.\n",
    "\n",
    "**Tool Strategy:**\n",
    "- For simple tasks (1-2 tools), call tools directly\n",
    "- For complex tasks (multiple tools, comparisons, logic), use execute_code\n",
    "\n",
    "Use print() in execute_code for output.\"\"\"\n",
    "\n",
    "async def run_agent(query: str):\n",
    "    \"\"\"Simple agent loop that handles tool calls.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        response = await litellm.acompletion(model=\"gpt-4o\", messages=messages, tools=tools)\n",
    "        msg = response.choices[0].message\n",
    "        \n",
    "        if not msg.tool_calls:\n",
    "            return msg.content\n",
    "        \n",
    "        messages.append(msg.model_dump())\n",
    "        \n",
    "        for tc in msg.tool_calls:\n",
    "            name = tc.function.name\n",
    "            args = json.loads(tc.function.arguments)\n",
    "            print(f\"  → Calling: {name}({args})\")\n",
    "            \n",
    "            # Execute the tool\n",
    "            if name == \"execute_code\":\n",
    "                result = await ptc_tool.run(args[\"code\"])\n",
    "            elif name == \"get_weather\":\n",
    "                result = await get_weather.tool.execute(**args)\n",
    "            elif name == \"search_hotels\":\n",
    "                result = await search_hotels.tool.execute(**args)\n",
    "            elif name == \"book_hotel\":\n",
    "                result = await book_hotel.tool.execute(**args)\n",
    "            \n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": str(result)})\n",
    "            print(f\"  ← Result: {result[:100]}...\" if len(str(result)) > 100 else f\"  ← Result: {result}\")\n",
    "\n",
    "print(\"Agent ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Simple Task (Direct Call)\n",
    "\n",
    "For simple queries, the LLM will call tools directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await run_agent(\"What's the weather in Tokyo?\")\n",
    "print(f\"\\nFinal: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Complex Task (PTC)\n",
    "\n",
    "For complex queries requiring multiple tools and logic, the LLM will use `execute_code` to generate Python that orchestrates everything in one pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await run_agent(\n",
    "    \"Check weather in Tokyo, London, and Paris. Find hotels under $150 in the warmest city and book the cheapest one.\"\n",
    ")\n",
    "print(f\"\\nFinal: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Happened?\n",
    "\n",
    "**Simple task:** The LLM called `get_weather` directly → got result → responded.\n",
    "\n",
    "**Complex task:** The LLM used `execute_code` and generated something like:\n",
    "\n",
    "```python\n",
    "# Get weather for all cities\n",
    "tokyo = await get_weather(\"Tokyo\")      # \"72°F sunny\"\n",
    "london = await get_weather(\"London\")    # \"55°F cloudy\"  \n",
    "paris = await get_weather(\"Paris\")      # \"60°F rainy\"\n",
    "\n",
    "# Find warmest (Tokyo at 72°F)\n",
    "warmest = \"Tokyo\"\n",
    "\n",
    "# Search hotels\n",
    "hotels = await search_hotels(\"Tokyo\", max_price=150)\n",
    "\n",
    "# Book cheapest\n",
    "result = await book_hotel(hotels[0])\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**One inference pass instead of 6+ tool calls!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "```python\n",
    "from polytool import create_execute_code_tool, tool\n",
    "import litellm\n",
    "\n",
    "# 1. Define tools\n",
    "@tool\n",
    "async def my_tool(x: str) -> str: ...\n",
    "\n",
    "# 2. Create PTC tool\n",
    "ptc = create_execute_code_tool(tools=[my_tool])\n",
    "\n",
    "# 3. Use with any LLM\n",
    "response = await litellm.acompletion(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[ptc.schema, my_tool.tool.to_openai_schema()],\n",
    "    messages=[...]\n",
    ")\n",
    "\n",
    "# 4. Execute PTC if called\n",
    "if tool_call.name == \"execute_code\":\n",
    "    result = await ptc.run(tool_call.arguments[\"code\"])\n",
    "```\n",
    "\n",
    "The LLM decides when to use PTC vs direct calls based on task complexity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
